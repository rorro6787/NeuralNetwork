\documentclass[../main]{subfiles}
\usepackage{lastpage,xr,refcount,etoolbox}
%\externaldocument{../Appendices/Appendix1-CodigosBase}
\begin{document}


\chapter{The Backpropagation Algorithm}

{
\hypersetup{linkcolor=black}
\minitoc
\vspace{5mm}
}
The backpropagation algorithm is a fundamental technique used in training artificial neural networks. It is a supervised learning method that adjusts the weights of the network to minimize the error in its predictions. This is achieved by propagating the error backward from the output layer to the input layer. The process involves two main phases: forward propagation, where the input data passes through the network to produce an output, and backward propagation, where the error is calculated and propagated back through the network. During backward propagation, the algorithm uses the chain rule of calculus to compute the gradient of the loss function with respect to each weight, allowing for the precise adjustment of weights to reduce the error.

In practical terms, backpropagation iteratively updates the network's weights by using gradient descent or a variant like stochastic gradient descent (SGD). After each iteration, or epoch, the weights are adjusted in the direction that reduces the error. This process continues until the network's predictions are sufficiently accurate or until another stopping criterion is met, such as a fixed number of epochs. Backpropagation has been instrumental in enabling deep learning, allowing neural networks with many layers (deep neural networks) to be trained effectively. Despite its effectiveness, it requires careful tuning of hyperparameters, such as the learning rate, to ensure convergence and avoid issues like overfitting.

In the context of what we have explained so far, backpropagation is the crucial tool we need to create to complete our working neural network. Before diving into the backpropagation equations, we need to mention two things:

\section{The Hadamard product}
The implementation of backpropagation involves a series of linear algebraic and matrix operations. One particularly useful operation is the Hadamard product, which is a type of matrix multiplication where each element is multiplied element-wise. We denote this operation with the $\odot$ symbol:
\begin{equation*}
\begin{pmatrix}
a\vspace{0.1cm}\\ 
b\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
h
\end{pmatrix} \odot \begin{pmatrix}
i\vspace{0.1cm}\\ 
j\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
t
\end{pmatrix} = \begin{pmatrix}
a*i\vspace{0.1cm}\\ 
b*j\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
h*t
\end{pmatrix}
\end{equation*}
\section{The error of a neuron}
As previously discussed, backpropagation is an efficient algorithm that updates the weights and biases of our network based on how the cost function changes. To achieve this, we introduce an intermediate value known as the error of the neuron. This error is defined as the rate of change of the weighted input to the neuron with respect to the cost function:
\begin{equation*}
    \delta_{j}^{l} = \frac{\partial C}{\partial z_{j}^{l}}
\end{equation*}
Note that $z_{j}^{l}$ is the weighted input of the neuron j in the $l^th$ layer. We use that instead of the output activation to because is algebraically easy to understand:
\begin{equation*}
    z_{j}^{l} = \sum_k w_{j,k}^{l}\cdot a_{k}^{l-1} + b_{j}^l
\end{equation*}
\begin{equation*}
    a_{j}^{l} = \sigma (z_{j}^{l})
\end{equation*}
\section{The equations of Backpropagation}
Backpropagation is based on four fundamental equations. These equations together allow us to compute both the error $\delta^l$ and the gradient of the cost function. For each of these four equations, I will first present the equation itself, and you can optionally refer to its proof if needed:
\subsection{First equation}
The initial step in backpropagation is to compute the error in the output layer. This expression provides the error associated with a given input in the output layer, which is essential for propagating the error backward through the network:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \delta_{j}^L = \frac{\partial C}{\partial a_{j}^L}\cdot\sigma'(z_{j}^L)
\end{equation*}
\end{mdframed}\vspace{4mm}
Here is the element-wise form of the first equation. From this, we can also derive the matrix form of the same equation:
\begin{equation*}
    \delta^L = \nabla C(a) \odot \sigma'(z^L)
\end{equation*}
There is considerable debate about the best loss function to use, but for the purposes of this explanation, we will use the quadratic cost function, also known as Mean Squared Error (MSE):
\begin{equation*}
    C(a_{j}^L) = \frac{1}{2}\cdot\sum_{j}(y_{j}-a_{j}^L)^2
\end{equation*}
\begin{equation*}
    \frac{\partial C}{\partial a_{j}^L} = (a_{j}^L-y_j)
\end{equation*}
With all of this, we can finally derive an explicit formula for the matrix form of the equation using Mean Squared Error (MSE) as the loss function:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \delta^L = (a^L-y)\odot\sigma'(z^L)
\end{equation*}
\end{mdframed}
\subsubsection{Proof of the first equation}
Our goal now is to show that we have derived the equation that computes the error of the output layer:
\begin{equation*}
\delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \cdot \sigma'(z_{j}^L)
\end{equation*}
In order to prove it, we first recall the definition of the error associated with the weighted input of a neuron:
\begin{equation*}
\delta_{j}^L = \frac{\partial C}{\partial z_{j}^L}
\end{equation*}
We use the chain rule to express the partial derivative above in terms of partial derivatives with respect to the output activation:
\begin{equation*}
    \delta_{j}^L = \sum_k \frac{\partial C}{\partial a_{k}^L} \frac{\partial a_{k}^L}{\partial z_{j}^L}
\end{equation*}
The sum is across all the neurons in the output layer, but since the output activation \(a_{k}^L\) only depends on the weighted input \(z_{j}^L\) for the $j^th$ neuron when \(k = j\), the sum disappears, and we can rewrite it as:
\begin{equation*}
    \delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \frac{\partial a_{j}^L}{\partial z_{j}^L}
\end{equation*}
We defined above that \(a_{j}^L = \sigma(z_{j}^L)\), therefore we can rewrite the second term of the expression and obtain the final equation that completes the proof:
\begin{equation*}
    \delta_{j}^L = \frac{\partial C}{\partial a_{j}^L} \cdot \sigma'(z_{j}^L)
\end{equation*}
\subsection{Second equation}
Now that we have an equation that computes the error of the output layer, we need an equation that computes the error of a given layer in terms of the error of the next one (until we reach the output layer):\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \delta^l = \left((w^{l+1})^T \cdot \delta^{l+1}\right) \odot \sigma'(z^l)
\end{equation*}
\end{mdframed}\vspace{4mm}
This is the matrix form of the equation, but in the same way as in the previous equation, we can obtain the element form of the equation:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \delta_{j}^l = \sum_{k} w_{k,j}^{l+1} \cdot \delta_{k}^{l+1} \cdot \sigma'(z_{j}^l)
\end{equation*}
\end{mdframed}
\subsubsection{Proof of the second equation}
Our goal now is to show that we have derived the equation that computes the error of a given layer in terms of the error of the next one:
\begin{equation*}
    \delta^l = \left((w^{l+1})^T \cdot \delta^{l+1}\right) \odot \sigma'(z^l)
\end{equation*}
As before, we recall the definition and use the chain rule to rewrite the error in layer \(l\) in terms of the error in the next layer:
\begin{equation*}
    \delta_{j}^l = \frac{\partial C}{\partial z_{j}^l} = \sum_k \frac{\partial C}{\partial z_{k}^{l+1}} \frac{\partial z_{k}^{l+1}}{\partial z_{j}^l} = \sum_k \frac{\partial z_{k}^{l+1}}{\partial z_{j}^l} \cdot \delta_{k}^{l+1}
\end{equation*}
\begin{equation*}
    z_{k}^{l+1} = \sum_j w_{k,j}^{l+1} \cdot a_{j}^{l} + b_{k}^{l+1} = \sum_j w_{k,j}^{l+1} \cdot \sigma(z_{j}^{l}) + b_{k}^{l+1}
\end{equation*}
\begin{equation*}
    \frac{\partial z_{k}^{l+1}}{\partial z_{j}^l} = w_{k,j}^{l+1} \cdot \sigma'(z_{j}^{l})
\end{equation*}
\begin{equation*}
    \delta_{j}^l = \sum_k w_{k,j}^{l+1} \cdot \delta_{k}^{l+1} \cdot \sigma'(z_{j}^{l})
\end{equation*}
After performing all these transformations, we end up with the element version of the equation that we wanted to prove.
\subsection{Third equation}
Once we have the equations needed to compute the error in all the layers, we need an equation to compute the rate of change of the cost with respect to the biases:\\
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \frac{\partial C}{\partial b_{j}^l} = \delta_{j}^l
\end{equation*}
\end{mdframed}\vspace{4mm}
In the case of the biases, the error is exactly the rate of change. From this equation, we can obtain its matrix form:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \frac{\partial C}{\partial b^l} = \delta^l
\end{equation*}
\end{mdframed}
\subsubsection{Proof of the third equation}
Our goal now is to show that we have derived the equation that computes the the rate of change of the cost with respect to the biases:
\begin{equation*}
    \frac{\partial C}{\partial b_{j}^l} = \delta_{j}^l
\end{equation*}
In this case, to find $\frac{\partial C}{\partial b_{j}^l}$, we can use the chain rule. The relationship between the biases and the cost function can be expressed through the chain rule as follows:
\begin{equation*}
    \frac{\partial C}{\partial b_{j}^l} = \frac{\partial C}{\partial z_{j}^l}\cdot\frac{\partial z_{j}^l}{\partial b_{j}^l}
\end{equation*}
\begin{equation*}
    z_{k}^{l} = \sum_j w_{k,j}^{l} \cdot a_{j}^{l-1} + b_{k}^{l} 
\end{equation*}
\begin{equation*}
    \frac{\partial z_{k}^{l}}{\partial b_{j}^l} = 1
\end{equation*}
\begin{equation*}
    \frac{\partial C}{\partial b_{j}^l} = \frac{\partial C}{\partial z_{j}^l}=\delta_{j}^l
\end{equation*}
\subsection{Fourth equation}
Lastly, similar to the biases, we need an equation that computes the rate of change of the cost with respect to the weights:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \frac{\partial C}{\partial w_{j,k}^l} = \delta^l \cdot (a^{l-1})^T
\end{equation*}
\end{mdframed}\vspace{4mm}
In the case of the weights, the partial derivatives are computed from quantities of the error and the activation of the previous neuron. From this equation, we can obtain its matrix form:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{equation*}
    \frac{\partial C}{\partial w^l} = a^{l-1} \cdot \delta^l
\end{equation*}
\end{mdframed}
\subsubsection{Proof of the fourth equation}
Our goal now is to show that we have derived the equation that computes the the rate of change of the cost with respect to the weights:
\begin{equation*}
\frac{\partial C}{\partial w_{j,k}^l} = a_{k}^{l-1} \cdot \delta_{j}^l\end{equation*}
In this case, to find $\frac{\partial C}{\partial w_{j,k}^l}$, we can use the chain rule. The relationship between the weights and the cost function can be expressed through the chain rule as follows:
\begin{equation*}
    \frac{\partial C}{\partial w_{j,k}^l} = \frac{\partial C}{\partial z_{j}^l}\cdot\frac{\partial z_{j}^l}{\partial w_{j,k}^l}
\end{equation*}
\begin{equation*}
    z_{k}^{l} = \sum_j w_{k,j}^{l} \cdot a_{j}^{l-1} + b_{k}^{l} 
\end{equation*}
\begin{equation*}
    \frac{\partial z_{k}^{l}}{\partial w_{j,k}^l} = a_{j}^{l-1}
\end{equation*}
\begin{equation*}
    \frac{\partial C}{\partial w_{j,k}^l} = a_{j}^{l-1}\cdot\frac{\partial C}{\partial z_{j}^l}=a_{j}^{l-1}\cdot\delta_{j}^l
\end{equation*}
\section{The algorithm}
We have already shown and proved each of the equations of the backpropagation algorithm. I will now present them together in matrix form, as this is what we will use in our implementation:\vspace{6mm}
\begin{mdframed}[style=myequationstyle]
\begin{center}
    $\delta^L = (a^L-y)\odot\sigma'(z^L)$\\ \vspace{3mm}
    $\delta^l = \left((w^{l+1})^T \cdot \delta^{l+1}\right) \odot \sigma'(z^l)$\\ \vspace{3mm}
    $\frac{\partial C}{\partial b^l} = \delta^l$\\ \vspace{3mm}
    $\frac{\partial C}{\partial w^l} = \delta^l \cdot (a^{l-1})^T $
\end{center}
\end{mdframed}\vspace{4mm}
With all the equations written, we can now provide a high-level implementation of the backpropagation algorithm:
\begin{itemize}
    \item[\textbf{1.}] \textbf{Forward propagation }: Given a training input x with the desired output y, compute the weighted input of the output layer using:
    \begin{center}
        $\forall l \in [2, 3, ..., L]$ \\ \vspace{1.5mm}
        $z^l=w^l\cdot a^{l-1}+b^l$ \\ \vspace{1mm}
        $a^l = \sigma(z^l)$
    \end{center}
    \item[\textbf{2.}] \textbf{Output layer error}: The first step in the backpropagation algorithm is to calculate the error in the output layer associated with the processed training input (remember that we are using MSE as the loss function; if we chose another one, this step would change):
    \begin{center}
        $\delta^L = \left(\begin{pmatrix}
\sigma(z_{1}^L)\vspace{0.1cm}\\ 
\sigma(z_{2}^L)\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
\sigma(z_{n}^L)
\end{pmatrix}-\begin{pmatrix}
y_1\vspace{0.1cm}\\ 
y_2\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
y_n
\end{pmatrix}\right)\odot\begin{pmatrix}
\sigma'(z_{1}^L)\vspace{0.1cm}\\ 
\sigma'(z_{2}^L)\vspace{0.1cm}\\ 
...\vspace{0.1cm}\\
\sigma(z_{n}^L)
\end{pmatrix}$
    \end{center}
    \item[\textbf{3.}] \textbf{Backpropagate the error}: This is the step that gives the algorithm its name. Using the base case of the error calculated in the previous step, we backpropagate the error across all the layers by using the formula that computes the error of an arbitrary layer in terms of the next one:
    \begin{center}
        $ \forall l \in [2, 3, ..., L-1]$ \\ \vspace{2mm}
        $\delta^l = \left(\begin{pmatrix}
        w_{1,1}^{l+1} & w_{2,1}^{l+1} & ... & w_{n,1}^{l+1}\vspace{0.15cm}\\ 
        w_{1,2}^{l+1} & w_{2,2}^{l+1} & ... & w_{n,2}^{l+1}\vspace{0.15cm}\\ 
        ...       & ...       & ... & ...\vspace{0.15cm}\\
        w_{1,m}^{l+1} & w_{2,m}^{l+1} & ... & w_{n,m}^{l+1}
        \end{pmatrix}^T\cdot
        \begin{pmatrix}
        \delta_{1}^{l+1} \vspace{0.15cm}\\ 
        \delta_{2}^{l+1} \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        \delta_{m}^{l+1}
        \end{pmatrix}\right) \odot \begin{pmatrix}
        \sigma'(z_{1}^l) \vspace{0.15cm}\\ 
        \sigma'(z_{2}^l) \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        \sigma'(z_{n}^l)
        \end{pmatrix}$
    \end{center}
    \item[\textbf{4.}] \textbf{Gradient of the cost function}: The final step is to compute the gradient of the cost function with respect to the weights and biases:
    \begin{center}
        $ \frac{\partial C}{\partial w^l} = \begin{pmatrix}
        \frac{\partial C}{\partial w_{1,1}^{l}} & \frac{\partial C}{\partial w_{2,1}^{l}} & ... & \frac{\partial C}{\partial w_{n,1}^{l}}\vspace{0.15cm}\\ 
        \frac{\partial C}{\partial w_{1,2}^{l}} & \frac{\partial C}{\partial w_{2,2}^{l}} & ... & \frac{\partial C}{\partial w_{n,2}^{l}}\vspace{0.15cm}\\ 
        ...       & ...       & ... & ...\vspace{0.15cm}\\
        \frac{\partial C}{\partial w_{1,m}^{l}} & \frac{\partial C}{\partial w_{2,m}^{l}} & ... & \frac{\partial C}{\partial w_{n,m}^{l}}
        \end{pmatrix}
        =\begin{pmatrix}
        \delta_{1}^{l} \vspace{0.15cm}\\ 
        \delta_{2}^{l} \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        \delta_{m}^{l} 
        \end{pmatrix}\cdot 
        \begin{pmatrix}
        a_{1}^{l-1} \vspace{0.15cm}\\ 
        a_{2}^{l-1} \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        a_{n}^{l-1}
        \end{pmatrix}^T
        $ \quad \quad
        $ \frac{\partial C}{\partial b^l} = \begin{pmatrix}
        \frac{\partial C}{\partial b_{1}^{l}} \vspace{0.15cm}\\ 
        \frac{\partial C}{\partial b_{2}^{l}} \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        \frac{\partial C}{\partial b_{m}^{l}}
        \end{pmatrix}
        = \begin{pmatrix}
        \delta_{1}^{l} \vspace{0.15cm}\\ 
        \delta_{2}^{l} \vspace{0.15cm}\\ 
        ...     \vspace{0.15cm}\\
        \delta_{m}^{l} 
        \end{pmatrix}
        $
    \end{center}
\end{itemize}
\section{The code implementation}
We have already established all the mathematical tools required to implement our own neural network. It's now time to jump into the code. First, we need to implement the auxiliary functions that backpropagation requires: the derivative of the sigmoid function and the derivative of the loss function (which returns the vector of partial derivatives for the output activation, in this case, the MSE):
\begin{lstlisting}
def sigmoid_prime(z):
    return sigmoid(z)*(1-sigmoid(z))

def cost_derivative(self, output_activations, y):
    return (output_activations-y) 
\end{lstlisting}
Finally, we can move on to the implementation of the backpropagation algorithm. To recap, the algorithm receives a training input with its desired output and returns the gradient of the cost function with respect to both the weights and biases:
\begin{lstlisting}
def backprop(self, x, y):
    nabla_b = [np.zeros(b.shape) for b in self.biases]
    nabla_w = [np.zeros(w.shape) for w in self.weights]
        
    # Forward Propagation
    activation = x
    activations = [x] 
    zs = [] 
    for b, w in zip(self.biases, self.weights):
        z = np.dot(w, activation)+b
        zs.append(z)
        activation = sigmoid(z)
        activations.append(activation)
    # Output layer error
    delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])
    nabla_b[-1] = delta
    nabla_w[-1] = np.dot(delta, activations[-2].transpose())

    # Backpropagate the error
    for l in range(2, self.num_layers):
        z = zs[-l]
        sp = sigmoid_prime(z)
        delta = np.dot(self.weights[-l+1].transpose(), delta) * sp
        # Gradient of the cost function
        nabla_b[-l] = delta
        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())
    return (nabla_b, nabla_w)
\end{lstlisting}
\end{document}



% \stackrel{2}{1} B

% {\textcolor{blue}{\ref{fig:concepto}}}